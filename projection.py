from seobuk_251001A import run_pipeline
import traceback
import logging
from urllib.parse import urlparse

# Configure logging only if not already configured
if not logging.getLogger().hasHandlers():
    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

def execute_crawling(waiting_list, gcp_secrets, spreadsheet_name):
    """
    ëŒ€ê¸° ì¤‘ ì‘ì—… ëª©ë¡ì„ ë°›ì€ í›„ í¬ë¡¤ë§ì„ ì‹¤í–‰í•˜ê³  ê²°ê³¼ë¥¼ ë°˜í™˜.
    Args:
        waiting_list (list): ëŒ€ê¸° ì¤‘ ì‘ì—… ë¦¬ìŠ¤íŠ¸ (sales_team, url, buyer)
        gcp_secrets (dict): GCP Service Account ì¸ì¦ ì •ë³´
        spreadsheet_name (str): ì‘ì—… ëŒ€ìƒ Google ìŠ¤í”„ë ˆë“œì‹œíŠ¸ ì´ë¦„
    Returns:
        list: ì™„ë£Œëœ ì‘ì—…ì˜ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
    """
    logging.info(f"[execute_crawling] ì‹œì‘")
    logging.info(f"   - waiting_list ê°œìˆ˜: {len(waiting_list)}")
    logging.info(f"   - gcp_secrets íƒ€ì…: {type(gcp_secrets)}")
    logging.info(f"   - spreadsheet_name: {spreadsheet_name}")
    
    print(f"ğŸš€ [DEBUG] execute_crawling ì‹œì‘")
    print(f"   - waiting_list ê°œìˆ˜: {len(waiting_list)}")
    print(f"   - gcp_secrets ì¡´ì¬ ì—¬ë¶€: {gcp_secrets is not None}")
    print(f"   - spreadsheet_name: {spreadsheet_name}")
    
    # Validate inputs
    if not waiting_list:
        logging.error("[execute_crawling] waiting_listê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤")
        print(f"âŒ [ERROR] waiting_listê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤")
        return []
    
    if not gcp_secrets:
        logging.error("[execute_crawling] gcp_secretsê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤")
        print(f"âŒ [ERROR] gcp_secretsê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤")
        return []
    
    if not spreadsheet_name:
        logging.error("[execute_crawling] spreadsheet_nameì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤")
        print(f"âŒ [ERROR] spreadsheet_nameì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤")
        return []
    
    completed_tasks = []

    for idx, task in enumerate(waiting_list):
        try:
            logging.info(f"[execute_crawling] ì‘ì—… {idx+1}/{len(waiting_list)} ì²˜ë¦¬ ì¤‘")
            print(f"\nğŸš€ [DEBUG] ì‘ì—… {idx+1}/{len(waiting_list)} ì²˜ë¦¬ ì¤‘")
            print(f"   - URL: {task.get('url', 'N/A')}")
            print(f"   - Buyer: {task.get('buyer', 'N/A')}")
            print(f"   - SalesíŒ€: {task.get('sales_team', 'N/A')}")
            
            # Validate inputs
            missing_fields = []
            if not task.get("url"):
                missing_fields.append("URL")
            if not task.get("buyer"):
                missing_fields.append("Buyer")
            
            if missing_fields:
                error_msg = f"{', '.join(missing_fields)}ê°€ ì—†ìŠµë‹ˆë‹¤"
                logging.error(f"[execute_crawling] {error_msg}")
                print(f"âŒ [ERROR] {error_msg}")
                completed_tasks.append({
                    "url": task.get("url", "N/A"),
                    "buyer": task.get("buyer", "N/A"),
                    "status": "FAILED",
                    "error": error_msg
                })
                continue
            
            # Validate URL format (use local variable to avoid mutating input)
            url = task["url"].strip()
            
            try:
                parsed_url = urlparse(url)
                if not parsed_url.scheme or not parsed_url.netloc:
                    error_msg = "ìœ íš¨í•˜ì§€ ì•Šì€ URL í˜•ì‹ì…ë‹ˆë‹¤ (ë„ë©”ì¸ì´ ì—†ê±°ë‚˜ í”„ë¡œí† ì½œì´ ëˆ„ë½ë¨)"
                    logging.error(f"[execute_crawling] {error_msg}: {url}")
                    print(f"âŒ [ERROR] {error_msg}: {url}")
                    completed_tasks.append({
                        "url": url,
                        "buyer": task.get("buyer", "N/A"),
                        "status": "FAILED",
                        "error": error_msg
                    })
                    continue
                if parsed_url.scheme not in ("http", "https"):
                    error_msg = "URLì€ http:// ë˜ëŠ” https://ë¡œ ì‹œì‘í•´ì•¼ í•©ë‹ˆë‹¤"
                    logging.error(f"[execute_crawling] {error_msg}: {url}")
                    print(f"âŒ [ERROR] {error_msg}: {url}")
                    completed_tasks.append({
                        "url": url,
                        "buyer": task.get("buyer", "N/A"),
                        "status": "FAILED",
                        "error": error_msg
                    })
                    continue
            except Exception as e:
                error_msg = f"URL íŒŒì‹± ì‹¤íŒ¨: {str(e)}"
                logging.error(f"[execute_crawling] {error_msg}")
                print(f"âŒ [ERROR] {error_msg}")
                completed_tasks.append({
                    "url": url,
                    "buyer": task.get("buyer", "N/A"),
                    "status": "FAILED",
                    "error": error_msg
                })
                continue
            
            # Use cleaned URL for crawling
            list_pairs = [(url, task["buyer"])]
            logging.info(f"[execute_crawling] run_pipeline í˜¸ì¶œ ì¤‘...")
            print(f"   - run_pipeline í˜¸ì¶œ ì¤‘...")
            records = run_pipeline(
                list_pairs=list_pairs,
                user_name=task["sales_team"],
                gcp_secrets=gcp_secrets,
                spreadsheet_name=spreadsheet_name,
                headless=True
            )
            logging.info(f"[execute_crawling] run_pipeline ë°˜í™˜ê°’: {records}")
            print(f"   - run_pipeline ë°˜í™˜ê°’: {records}")
            
            if records:
                completed_tasks.extend(records)
                logging.info(f"[execute_crawling] ì‘ì—… ì„±ê³µ - {len(records)}ê°œ ë ˆì½”ë“œ ì¶”ê°€")
                print(f"âœ… [DEBUG] ì‘ì—… ì„±ê³µ - {len(records)}ê°œ ë ˆì½”ë“œ ì¶”ê°€")
            else:
                logging.warning("[execute_crawling] run_pipelineì´ ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜")
                print(f"âš ï¸  [WARNING] run_pipelineì´ ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜")
                completed_tasks.append({
                    "url": task["url"],
                    "buyer": task["buyer"],
                    "status": "FAILED",
                    "error": "í¬ë¡¤ë§ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤"
                })
        except Exception as e:
            error_msg = f"ì‘ì—… ì‹¤íŒ¨: {str(e)}"
            logging.error(f"[execute_crawling] {error_msg}")
            logging.error(traceback.format_exc())
            print(f"âŒ [ERROR] {error_msg}")
            print(traceback.format_exc())
            completed_tasks.append({
                "url": task.get("url", "N/A"),
                "buyer": task.get("buyer", "N/A"),
                "status": "FAILED",
                "error": str(e)
            })
    
    logging.info(f"[execute_crawling] ì¢…ë£Œ - ì´ ì²˜ë¦¬ëœ ì‘ì—…: {len(completed_tasks)}")
    print(f"\nğŸš€ [DEBUG] execute_crawling ì¢…ë£Œ")
    print(f"   - ì´ ì²˜ë¦¬ëœ ì‘ì—…: {len(completed_tasks)}")
    print(f"   - completed_tasks: {completed_tasks}")
    return completed_tasks
